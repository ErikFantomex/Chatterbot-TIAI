{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRFHVStU3K8OXc5WABSj5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErikFantomex/Chatterbot-TIAI/blob/main/Chatbot_TAIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "U_GL2agb7Ge5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatterbot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhNbicu98_Bg",
        "outputId": "08025183-98a4-4c13-ef47-fd0ca06acb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chatterbot\n",
            "  Downloading ChatterBot-1.0.5-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from chatterbot) (2022.6)\n",
            "Collecting pint>=0.8.1\n",
            "  Downloading Pint-0.20.1-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 48.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml<5.2,>=5.1\n",
            "  Downloading PyYAML-5.1.2.tar.gz (265 kB)\n",
            "\u001b[K     |████████████████████████████████| 265 kB 66.0 MB/s \n",
            "\u001b[?25hCollecting python-dateutil<2.8,>=2.7\n",
            "  Downloading python_dateutil-2.7.5-py2.py3-none-any.whl (225 kB)\n",
            "\u001b[K     |████████████████████████████████| 225 kB 68.0 MB/s \n",
            "\u001b[?25hCollecting sqlalchemy<1.3,>=1.2\n",
            "  Downloading SQLAlchemy-1.2.19.tar.gz (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 49.0 MB/s \n",
            "\u001b[?25hCollecting spacy<2.2,>=2.1\n",
            "  Downloading spacy-2.1.9.tar.gz (30.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.7 MB 1.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk<4.0,>=3.2 in /usr/local/lib/python3.8/dist-packages (from chatterbot) (3.7)\n",
            "Collecting pymongo<4.0,>=3.3\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/7d/b5/5fca287104815b858ec5b46b07e442d9d6ba51342ace3bb8f0b849059ecc/pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 31.1 MB/s \n",
            "\u001b[?25hCollecting mathparse<0.2,>=0.1\n",
            "  Downloading mathparse-0.1.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (2022.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<2.8,>=2.7->chatterbot) (1.15.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (0.10.1)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Using cached blis-0.2.4-cp38-cp38-linux_x86_64.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.0.9)\n",
            "Collecting srsly<1.1.0,>=0.0.6\n",
            "  Using cached srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "  Using cached thinc-7.0.8-cp38-cp38-linux_x86_64.whl\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.21.6)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Using cached preshed-2.0.1-cp38-cp38-linux_x86_64.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2.10)\n",
            "Building wheels for collected packages: pyyaml, spacy, sqlalchemy\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp38-cp38-linux_x86_64.whl size=44117 sha256=93195c8b52c6334a960d239266a0a11fd32041616b947e0bca227ba1d0a3de83\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/40/9f/027c3d94280ce2b7c2c107cb563a433e6572f830a5462231ae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from chatterbot import ChatBot\n",
        "from chatterbot.trainers import ListTrainer"
      ],
      "metadata": {
        "id": "Aln2wID58Aqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-SsT3pI6_Pn",
        "outputId": "94fced0f-0582-4214-adbe-b248fec28a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "->  Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance – Google AI BlogSkip to main contentPathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough PerformancePathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance Philosophy Research Areas Publications People Tools & Downloads Outreach Careers Blog ArchiveLabelsAllABCDEFGHIJKLMNOPQRSTUVWXYZBlog          ›         Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough PerformanceMonday, April 04, 2022Posted by Sharan Narang and Aakanksha Chowdhery, Software Engineers, Google ResearchIn recent years, large neural networks trained for language understanding and generation have achieved impressive results across a wide range of tasks. \n",
            "\n",
            "->  GPT-3 first showed that large language models (LLMs) can be used for few-shot learning and can achieve impressive results without large-scale task-specific data collection or model parameter updating. \n",
            "\n",
            "->  More recent LLMs, such as GLaM, LaMDA, Gopher, and Megatron-Turing NLG, achieved state-of-the-art few-shot results on many tasks by scaling model size, using sparsely activated modules, and training on larger datasets from more diverse sources. \n",
            "\n",
            "->  Yet much work remains in understanding the capabilities that emerge with few-shot learning as we push the limits of model scale.Last year Google Research announced our vision for Pathways, a single model that could generalize across domains and tasks while being highly efficient. \n",
            "\n",
            "->  An important milestone toward realizing this vision was to develop the new Pathways system to orchestrate distributed computation for accelerators. \n",
            "\n",
            "->  In “PaLM: Scaling Language Modeling with Pathways”, we introduce the Pathways Language Model (PaLM), a 540-billion parameter, dense decoder-only Transformer model trained with the Pathways system, which enabled us to efficiently train a single model across multiple TPU v4 Pods. \n",
            "\n",
            "->  We evaluated PaLM on hundreds of language understanding and generation tasks, and found that it achieves state-of-the-art few-shot performance across most tasks, by significant margins in many cases. \n",
            "\n",
            "->  As the scale of the model increases, the performance improves across tasks while also unlocking new capabilities.Training a 540-Billion Parameter Language Model with PathwaysPaLM demonstrates the first large-scale use of the Pathways system to scale training to 6144 chips, the largest TPU-based system configuration used for training to date. \n",
            "\n",
            "->  The training is scaled using data parallelism at the Pod level across two Cloud TPU v4 Pods, while using standard data and model parallelism within each Pod. \n",
            "\n",
            "->  This is a significant increase in scale compared to most previous LLMs, which were either trained on a single TPU v3 Pod (e.g., GLaM, LaMDA), used pipeline parallelism to scale to 2240 A100 GPUs across GPU clusters (Megatron-Turing NLG) or used multiple TPU v3 Pods (Gopher) with a maximum scale of 4096 TPU v3 chips.PaLM achieves a training efficiency of 57.8% hardware FLOPs utilization, the highest yet achieved for LLMs at this scale. \n",
            "\n",
            "->  This is due to a combination of the parallelism strategy and a reformulation of the Transformer block that allows for attention and feedforward layers to be computed in parallel, enabling speedups from TPU compiler optimizations.PaLM was trained using a combination of English and multilingual datasets that include high-quality web documents, books, Wikipedia, conversations, and GitHub code. \n",
            "\n",
            "->  We also created a “lossless” vocabulary that preserves all whitespace (especially important for code), splits out-of-vocabulary Unicode characters into bytes, and splits numbers into individual tokens, one for each digit.Breakthrough Capabilities on Language, Reasoning, and Code TasksPaLM shows breakthrough capabilities on numerous very difficult tasks. \n",
            "\n",
            "->  We highlight a few examples for language understanding and generation, reasoning, and code-related tasks below.Language Understanding and GenerationWe evaluated PaLM on 29 widely-used English natural language processing (NLP) tasks. \n",
            "\n",
            "->  PaLM 540B surpassed few-shot performance of prior large models, such as GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, and LaMDA, on 28 of 29 of tasks that span question-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasks.PaLM 540B performance improvement over prior state-of-the-art (SOTA) results on 29 English-based NLP tasks.In addition to English NLP tasks, PaLM also shows strong performance on multilingual NLP benchmarks, including translation, even though only 22% of the training corpus is non-English.We also probe emerging and future capabilities of PaLM on the Beyond the Imitation Game Benchmark (BIG-bench), a recently released suite of more than 150 new language modeling tasks, and find that PaLM achieves breakthrough performance. \n",
            "\n",
            "->  We compare the performance of PaLM to Gopher and Chinchilla, averaged across a common subset of 58 of these tasks. \n",
            "\n",
            "->  Interestingly, we note that PaLM’s performance as a function of scale follows a log-linear behavior similar to prior models, suggesting that performance improvements from scale have not yet plateaued. \n",
            "\n",
            "->  PaLM 540B 5-shot also does better than the average performance of people asked to solve the same tasks.Scaling behavior of PaLM on a subset of 58 BIG-bench tasks. \n",
            "\n",
            "->  PaLM demonstrates impressive natural language understanding and generation capabilities on several BIG-bench tasks. \n",
            "\n",
            "->  For example, the model can distinguish cause and effect, understand conceptual combinations in appropriate contexts, and even guess the movie from an emoji.Examples that showcase PaLM 540B 1-shot performance on BIG-bench tasks: labeling cause and effect, conceptual understanding, guessing movies from emoji, and finding synonyms and counterfactuals.ReasoningBy combining model scale with chain-of-thought prompting, PaLM shows breakthrough capabilities on reasoning tasks that require multi-step arithmetic or common-sense reasoning. \n",
            "\n",
            "->  Prior LLMs, like Gopher, saw less benefit from model scale in improving performance.Standard prompting versus chain-of-thought prompting for an example grade-school math problem. \n",
            "\n",
            "->  Chain-of-thought prompting decomposes the prompt for a multi-step reasoning problem into intermediate steps (highlighted in yellow), similar to how a person would approach it.We observed strong performance from PaLM 540B combined with chain-of-thought prompting on three arithmetic datasets and two commonsense reasoning datasets. \n",
            "\n",
            "->  For example, with 8-shot prompting, PaLM solves 58% of the problems in GSM8K, a benchmark of thousands of challenging grade school level math questions, outperforming the prior top score of 55% achieved by fine-tuning the GPT-3 175B model with a training set of 7500 problems and combining it with an external calculator and verifier. \n",
            "\n",
            "->  This new score is especially interesting, as it approaches the 60% average of problems solved by 9-12 year olds, who are the target audience for the question set. \n",
            "\n",
            "->  We suspect that separate encoding of digits in the PaLM vocabulary helps enable these performance improvements.Remarkably, PaLM can even generate explicit explanations for scenarios that require a complex combination of multi-step logical inference, world knowledge, and deep language understanding. \n",
            "\n",
            "->  For example, it can provide high quality explanations for novel jokes not found on the web.PaLM explains an original joke with two-shot prompts.Code GenerationLLMs have also been shown [1, 2, 3, 4] to generalize well to coding tasks, such as writing code given a natural language description (text-to-code), translating code from one language to another, and fixing compilation errors (code-to-code).PaLM 540B shows strong performance across coding tasks and natural language tasks in a single model, even though it has only 5% code in the pre-training dataset. \n",
            "\n",
            "->  Its few-shot performance is especially remarkable because it is on par with the fine-tuned Codex 12B while using 50 times less Python code for training. \n",
            "\n",
            "->  This result reinforces earlier findings that larger models can be more sample efficient than smaller models because they transfer learning from both other programming languages and natural language data more effectively.Examples of a fine-tuned PaLM 540B model on text-to-code tasks, such as GSM8K-Python and HumanEval, and code-to-code tasks, such as Transcoder.We also see a further increase in performance by fine-tuning PaLM on a Python-only code dataset, which we refer to as PaLM-Coder. \n",
            "\n",
            "->  For an example code repair task called DeepFix, where the objective is to modify initially broken C programs until they compile successfully, PaLM-Coder 540B demonstrates impressive performance, achieving a compile rate of 82.1%, which outperforms the prior 71.7% state of the art. \n",
            "\n",
            "->  This opens up opportunities for fixing more complex errors that arise during software development.An example from the DeepFix Code Repair task. \n",
            "\n",
            "->  The fine-tuned PaLM-Coder 540B fixes compilation errors (left, in red) to a version of code that compiles (right).Ethical ConsiderationsRecent research has highlighted various potential risks associated with LLMs trained on web text. \n",
            "\n",
            "->  It is crucial to analyze and document such potential undesirable risks through transparent artifacts such as model cards and datasheets, which also include information on intended use and testing. \n",
            "\n",
            "->  To this end, our paper provides a datasheet, model card and Responsible AI benchmark results, and it reports thorough analyses of the dataset and model outputs for biases and risks. \n",
            "\n",
            "->  While the analysis helps outline some potential risks of the model, domain- and task-specific analysis is essential to truly calibrate, contextualize, and mitigate possible harms. \n",
            "\n",
            "->  Further understanding of risks and benefits of these models is a topic of ongoing research, together with developing scalable solutions that can put guardrails against malicious uses of language models. \n",
            "\n",
            "->  Conclusion and Future WorkPaLM demonstrates the scaling capability of the Pathways system to thousands of accelerator chips across two TPU v4 Pods by training a 540-billion parameter model efficiently with a well-studied, well-established recipe of a dense decoder-only Transformer model. \n",
            "\n",
            "->  Pushing the limits of model scale enables breakthrough few-shot performance of PaLM across a variety of natural language processing, reasoning, and code tasks. \n",
            "\n",
            "->  PaLM paves the way for even more capable models by combining the scaling capabilities with novel architectural choices and training schemes, and brings us closer to the Pathways vision: “Enable a single AI system to generalize across thousands or millions of tasks, to understand different types of data, and to do so with remarkable efficiency. \n",
            "\n",
            "->  \"AcknowledgementsPaLM is the result of a large, collaborative effort by many teams within Google Research and across Alphabet. \n",
            "\n",
            "->  We’d like to thank the entire PaLM team for their contributions: Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, and Jason Wei. \n",
            "\n",
            "->  PaLM builds on top of work by many, many teams at Google and we would especially like to recognize the T5X team, the Pathways infrastructure team, the JAX team, the Flaxformer team, the XLA team, the Plaque team, the Borg team, and the Datacenter networking infrastructure team. \n",
            "\n",
            "->  We’d like to thank our co-authors on this blog post, Alexander Spiridonov and Maysam Moussalem, as well as Josh Newlan and Tom Small for the images and animations in this blog post. \n",
            "\n",
            "->  Finally, we would like to thank our advisors for the project: Noah Fiedel, Slav Petrov, Jeff Dean, Douglas Eck, and Kathy Meier-Hellstern.Labels:Machine LearningNatural Language ProcessingSelf-Supervised LearningArchiveLabelsAllABCDEFGHIJKLMNOPQRSTUVWXYZPrivacyTermsAbout GoogleGoogle Products \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#iniciamos\n",
        "url = 'https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html'\n",
        "page = requests.get(url)\n",
        "page = page.text\n",
        "soup = BeautifulSoup(page, features=\"lxml\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.decompose()\n",
        "raw_text  = soup.get_text()\n",
        "\n",
        "cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(\\1>)\", \"\", raw_text.strip())\n",
        "cleaned = re.sub(r\"(?s)[\\n]?\", \"\", cleaned)\n",
        "cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
        "cleaned = re.sub(r\" \", \" \", cleaned)\n",
        "cleaned = re.sub(r\"  \", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\n\", \" \", cleaned)\n",
        "cleaned = cleaned.strip()\n",
        "\n",
        "tokens = nltk.sent_tokenize(cleaned)\n",
        "for token in tokens:\n",
        "  print('-> ', token, '\\n')\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "url = 'https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html'\n",
        "page = requests.get(url)\n",
        "page = page.text\n",
        "soup = BeautifulSoup(page, features=\"lxml\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.decompose()\n",
        "raw_text  = soup.get_text()\n",
        "\n",
        "cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(\\1>)\", \"\", raw_text.strip())\n",
        "cleaned = re.sub(r\"(?s)[\\n]?\", \"\", cleaned)\n",
        "cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
        "cleaned = re.sub(r\" \", \" \", cleaned)\n",
        "cleaned = re.sub(r\"  \", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\n\", \" \", cleaned)\n",
        "cleaned = re.sub(\"'\", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\\"\", \" \", cleaned)\n",
        "cleaned = cleaned.strip()\n",
        "     \n"
      ],
      "metadata": {
        "id": "QVzit9-s74GR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorizacion"
      ],
      "metadata": {
        "id": "d4j4-zYv7dCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = nltk.sent_tokenize(cleaned)\n",
        "word_tokens = nltk.word_tokenize(cleaned)"
      ],
      "metadata": {
        "id": "0wUo5OrA7KXS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "total_ocurrences = {}\n",
        "occured_in_n_docs = {}\n",
        "\n",
        "for sentence in sent_tokens:\n",
        "  for word in word_tokens:\n",
        " \n",
        "    c = sentence.count(word)\n",
        " \n",
        "    try:\n",
        "      total_ocurrences[word] += c\n",
        "    except KeyError:\n",
        "      total_ocurrences[word] = c \n",
        " \n",
        "    if c > 0:\n",
        "      try:\n",
        "        occured_in_n_docs[word] += 1\n",
        "      except KeyError:\n",
        "        occured_in_n_docs[word] = 1\n",
        " \n",
        "\n",
        "tfidf = []\n",
        "i = 0\n",
        "for sentence in sent_tokens:\n",
        "  w = {\"sentence_n\": i}\n",
        "  for word in word_tokens:\n",
        "    # w = tf * log(N/df)\n",
        "    tf = sentence.count(word)\n",
        "    N = total_ocurrences[word]\n",
        "    df = occured_in_n_docs[word]\n",
        "    if tf == 0:\n",
        "      w[word] = 0\n",
        "    else:\n",
        "      w[word] = tf * math.log(N/df)\n",
        "  tfidf.append(w)\n",
        "  i += 1\n"
      ],
      "metadata": {
        "id": "DrAOzTfU7wCG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(tfidf)\n",
        "     "
      ],
      "metadata": {
        "id": "ZGH_s91L78wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similaridad de coseno"
      ],
      "metadata": {
        "id": "2s18aIkx8Pp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html'\n",
        "page = requests.get(url)\n",
        "page = page.text\n",
        "soup = BeautifulSoup(page, features=\"lxml\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.decompose()\n",
        "raw_text  = soup.get_text()\n",
        "\n",
        "cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(\\1>)\", \"\", raw_text.strip())\n",
        "cleaned = re.sub(r\"(?s)[\\n]?\", \"\", cleaned)\n",
        "cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
        "cleaned = re.sub(r\" \", \" \", cleaned)\n",
        "cleaned = re.sub(r\"  \", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\n\", \" \", cleaned)\n",
        "cleaned = re.sub(\"'\", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\\"\", \" \", cleaned)\n",
        "cleaned = cleaned.strip()\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(cleaned)"
      ],
      "metadata": {
        "id": "zxhtZlOR8LXR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "similaridades = []\n",
        "for sentece in sent_tokens:\n",
        "  row = []\n",
        "  A = {w for w in nltk.word_tokenize(sentece)}\n",
        "  for another_sentence in sent_tokens:\n",
        "    l1 =[]\n",
        "    l2 =[]\n",
        "    B = {w for w in nltk.word_tokenize(another_sentence)}\n",
        "    v = A.union(B) \n",
        "    for w in v:\n",
        "      l1.append(1 if w in A else 0)\n",
        "      l2.append(1 if w in B else 0)\n",
        "    c = 0\n",
        "    for i in range(len(v)):\n",
        "        c+= l1[i]*l2[i]\n",
        "    row.append(c / float((sum(l1) * sum(l2)) ** 0.5))\n",
        "  similaridades.append(row)"
      ],
      "metadata": {
        "id": "oDouE19k8Uxr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(similaridades)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_bNokIF-8ZBP",
        "outputId": "2432f462-b74b-4154-c093-4f3f5681cb4a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6   \\\n",
              "0   1.000000  0.197814  0.162791  0.222911  0.107676  0.269880  0.250259   \n",
              "1   0.197814  1.000000  0.185164  0.197203  0.081650  0.146176  0.172516   \n",
              "2   0.162791  0.185164  1.000000  0.182574  0.037796  0.081200  0.287494   \n",
              "3   0.222911  0.197203  0.182574  1.000000  0.172516  0.247083  0.291606   \n",
              "4   0.107676  0.081650  0.037796  0.172516  1.000000  0.179029  0.042258   \n",
              "5   0.269880  0.146176  0.081200  0.247083  0.179029  1.000000  0.121046   \n",
              "6   0.250259  0.172516  0.287494  0.291606  0.042258  0.121046  1.000000   \n",
              "7   0.209381  0.173205  0.133631  0.292770  0.247487  0.303822  0.179284   \n",
              "8   0.096309  0.146059  0.202837  0.216025  0.089443  0.256205  0.151186   \n",
              "9   0.166839  0.161015  0.234255  0.194404  0.140859  0.302614  0.214286   \n",
              "10  0.140481  0.133156  0.147934  0.202567  0.163082  0.186857  0.165395   \n",
              "11  0.159749  0.161515  0.124611  0.182006  0.098907  0.188877  0.195047   \n",
              "12  0.268261  0.203419  0.156941  0.200574  0.083045  0.178410  0.386024   \n",
              "13  0.160567  0.206049  0.294817  0.174143  0.091766  0.131430  0.252059   \n",
              "14  0.242272  0.081650  0.188982  0.276026  0.150000  0.250640  0.380319   \n",
              "15  0.156486  0.101710  0.125553  0.200574  0.083045  0.178410  0.210559   \n",
              "16  0.144463  0.036515  0.101419  0.154303  0.134164  0.160128  0.226779   \n",
              "17  0.257396  0.195180  0.180702  0.206197  0.059761  0.085592  0.404061   \n",
              "18  0.113630  0.147710  0.159545  0.228869  0.060302  0.151142  0.254824   \n",
              "19  0.073721  0.149071  0.207020  0.157485  0.091287  0.098058  0.115728   \n",
              "20  0.165228  0.139212  0.154662  0.188248  0.136399  0.219774  0.201737   \n",
              "21  0.141999  0.107676  0.174456  0.204757  0.065938  0.165267  0.222911   \n",
              "22  0.111775  0.067806  0.125553  0.200574  0.166091  0.089205  0.175466   \n",
              "23  0.175762  0.177705  0.082261  0.250313  0.108821  0.129881  0.337227   \n",
              "24  0.162534  0.164330  0.171157  0.243047  0.100631  0.198174  0.233884   \n",
              "25  0.048154  0.109545  0.169031  0.185164  0.134164  0.096077  0.188982   \n",
              "26  0.152890  0.231869  0.257603  0.254755  0.056796  0.162690  0.288009   \n",
              "27  0.112807  0.057027  0.052796  0.120491  0.104765  0.150047  0.118056   \n",
              "28  0.051333  0.116775  0.108112  0.131590  0.143019  0.068279  0.080582   \n",
              "29  0.158330  0.150075  0.111154  0.177571  0.073521  0.210599  0.186411   \n",
              "30  0.091003  0.103510  0.223607  0.145803  0.084515  0.151307  0.142857   \n",
              "31  0.182006  0.172516  0.159719  0.262445  0.169031  0.151307  0.178571   \n",
              "32  0.122868  0.149071  0.138013  0.188982  0.136931  0.163430  0.154303   \n",
              "33  0.159256  0.207020  0.095831  0.233285  0.042258  0.121046  0.250000   \n",
              "34  0.180579  0.121716  0.197203  0.257172  0.186339  0.507072  0.188982   \n",
              "35  0.225920  0.190347  0.211472  0.353919  0.093250  0.233723  0.394055   \n",
              "36  0.238352  0.154919  0.191237  0.327327  0.221359  0.317038  0.213809   \n",
              "37  0.269191  0.122474  0.188982  0.310530  0.100000  0.179029  0.295804   \n",
              "38  0.072798  0.047316  0.043806  0.066649  0.077267  0.082998  0.081628   \n",
              "39  0.191533  0.064550  0.179284  0.245495  0.158114  0.198148  0.267261   \n",
              "40  0.109897  0.100000  0.154303  0.225374  0.204124  0.116941  0.207020   \n",
              "41  0.129732  0.065583  0.060718  0.166282  0.120483  0.172559  0.067884   \n",
              "\n",
              "          7         8         9   ...        32        33        34        35  \\\n",
              "0   0.209381  0.096309  0.166839  ...  0.122868  0.159256  0.180579  0.225920   \n",
              "1   0.173205  0.146059  0.161015  ...  0.149071  0.207020  0.121716  0.190347   \n",
              "2   0.133631  0.202837  0.234255  ...  0.138013  0.095831  0.197203  0.211472   \n",
              "3   0.292770  0.216025  0.194404  ...  0.188982  0.233285  0.257172  0.353919   \n",
              "4   0.247487  0.089443  0.140859  ...  0.136931  0.042258  0.186339  0.093250   \n",
              "5   0.303822  0.256205  0.302614  ...  0.163430  0.121046  0.507072  0.233723   \n",
              "6   0.179284  0.151186  0.214286  ...  0.154303  0.250000  0.188982  0.394055   \n",
              "7   1.000000  0.221359  0.239046  ...  0.193649  0.149404  0.368932  0.329690   \n",
              "8   0.221359  1.000000  0.277174  ...  0.244949  0.151186  0.366667  0.250217   \n",
              "9   0.239046  0.277174  1.000000  ...  0.154303  0.142857  0.230978  0.183892   \n",
              "10  0.161443  0.233384  0.238904  ...  0.208422  0.192961  0.218797  0.212905   \n",
              "11  0.163188  0.117954  0.130032  ...  0.090289  0.139320  0.098295  0.215206   \n",
              "12  0.146805  0.111417  0.163768  ...  0.113715  0.210559  0.123797  0.387202   \n",
              "13  0.178444  0.123117  0.232670  ...  0.146599  0.174503  0.136797  0.299504   \n",
              "14  0.318198  0.223607  0.225374  ...  0.273861  0.253546  0.298142  0.466252   \n",
              "15  0.205527  0.074278  0.163768  ...  0.151620  0.210559  0.154746  0.271041   \n",
              "16  0.252982  0.080000  0.151186  ...  0.163299  0.113389  0.166667  0.291920   \n",
              "17  0.126773  0.106904  0.067344  ...  0.109109  0.202031  0.133631  0.334367   \n",
              "18  0.170561  0.134840  0.135906  ...  0.137620  0.178377  0.134840  0.309277   \n",
              "19  0.161374  0.122474  0.180021  ...  0.125000  0.077152  0.102062  0.170251   \n",
              "20  0.192897  0.152499  0.211343  ...  0.155643  0.144098  0.203331  0.254385   \n",
              "21  0.186501  0.206419  0.185759  ...  0.180579  0.167183  0.270310  0.245950   \n",
              "22  0.176166  0.148556  0.187163  ...  0.189525  0.140372  0.154746  0.154881   \n",
              "23  0.179546  0.129777  0.143066  ...  0.198680  0.306570  0.162221  0.304430   \n",
              "24  0.213470  0.135011  0.198447  ...  0.137795  0.148835  0.168763  0.281516   \n",
              "25  0.221359  0.240000  0.176383  ...  0.122474  0.113389  0.133333  0.208514   \n",
              "26  0.180724  0.127000  0.160005  ...  0.155543  0.216007  0.169334  0.317776   \n",
              "27  0.197546  0.124939  0.177084  ...  0.191273  0.147570  0.182203  0.227951   \n",
              "28  0.101130  0.085280  0.107443  ...  0.087039  0.080582  0.071067  0.088911   \n",
              "29  0.155963  0.098639  0.248548  ...  0.201347  0.248548  0.164399  0.171398   \n",
              "30  0.179284  0.188982  0.142857  ...  0.308607  0.178571  0.157485  0.157622   \n",
              "31  0.209165  0.188982  0.166667  ...  0.270031  0.214286  0.220479  0.275839   \n",
              "32  0.193649  0.244949  0.154303  ...  1.000000  0.231455  0.238145  0.255377   \n",
              "33  0.149404  0.151186  0.142857  ...  0.231455  1.000000  0.188982  0.236433   \n",
              "34  0.368932  0.366667  0.230978  ...  0.238145  0.188982  1.000000  0.278019   \n",
              "35  0.329690  0.250217  0.183892  ...  0.255377  0.236433  0.278019  1.000000   \n",
              "36  0.290689  0.197990  0.231626  ...  0.173205  0.187083  0.353553  0.265396   \n",
              "37  0.212132  0.313050  0.197203  ...  0.273861  0.253546  0.298142  0.326377   \n",
              "38  0.068295  0.069109  0.054419  ...  0.088168  0.048977  0.071989  0.090065   \n",
              "39  0.167705  0.176777  0.155902  ...  0.216506  0.133631  0.235702  0.221163   \n",
              "40  0.144338  0.146059  0.184017  ...  0.186339  0.103510  0.152145  0.152277   \n",
              "41  0.141990  0.107763  0.090513  ...  0.146647  0.067884  0.119737  0.112351   \n",
              "\n",
              "          36        37        38        39        40        41  \n",
              "0   0.238352  0.269191  0.072798  0.191533  0.109897  0.129732  \n",
              "1   0.154919  0.122474  0.047316  0.064550  0.100000  0.065583  \n",
              "2   0.191237  0.188982  0.043806  0.179284  0.154303  0.060718  \n",
              "3   0.327327  0.310530  0.066649  0.245495  0.225374  0.166282  \n",
              "4   0.221359  0.100000  0.077267  0.158114  0.204124  0.120483  \n",
              "5   0.317038  0.179029  0.082998  0.198148  0.116941  0.172559  \n",
              "6   0.213809  0.295804  0.081628  0.267261  0.207020  0.067884  \n",
              "7   0.290689  0.212132  0.068295  0.167705  0.144338  0.141990  \n",
              "8   0.197990  0.313050  0.069109  0.176777  0.146059  0.107763  \n",
              "9   0.231626  0.197203  0.054419  0.155902  0.184017  0.090513  \n",
              "10  0.165027  0.228315  0.075605  0.154713  0.186418  0.130991  \n",
              "11  0.166812  0.131876  0.063685  0.130322  0.161515  0.105925  \n",
              "12  0.183829  0.166091  0.096250  0.164133  0.203419  0.100056  \n",
              "13  0.203133  0.183533  0.053179  0.145095  0.131122  0.073708  \n",
              "14  0.316228  0.350000  0.135217  0.276699  0.244949  0.160644  \n",
              "15  0.183829  0.166091  0.080208  0.196960  0.169516  0.100056  \n",
              "16  0.197990  0.178885  0.069109  0.212132  0.146059  0.071842  \n",
              "17  0.188982  0.119523  0.069264  0.188982  0.146385  0.048002  \n",
              "18  0.247900  0.120605  0.069890  0.143019  0.147710  0.096872  \n",
              "19  0.086603  0.091287  0.070535  0.108253  0.186339  0.109985  \n",
              "20  0.194099  0.170499  0.092217  0.215666  0.222738  0.164337  \n",
              "21  0.250217  0.230783  0.063685  0.208514  0.134595  0.079444  \n",
              "22  0.157568  0.249136  0.064167  0.196960  0.169516  0.100056  \n",
              "23  0.206474  0.217643  0.098097  0.172062  0.207322  0.116543  \n",
              "24  0.222756  0.150946  0.058316  0.119334  0.205412  0.101036  \n",
              "25  0.169706  0.134164  0.051832  0.141421  0.146059  0.071842  \n",
              "26  0.215526  0.198787  0.054856  0.202056  0.162309  0.091240  \n",
              "27  0.132518  0.209529  0.053965  0.138039  0.114053  0.084149  \n",
              "28  0.120605  0.095346  0.055253  0.075378  0.116775  0.076584  \n",
              "29  0.139497  0.147043  0.042606  0.145310  0.150075  0.059054  \n",
              "30  0.106904  0.169031  0.065302  0.167038  0.207020  0.101827  \n",
              "31  0.213809  0.253546  0.081628  0.167038  0.241523  0.169711  \n",
              "32  0.173205  0.273861  0.088168  0.216506  0.186339  0.146647  \n",
              "33  0.187083  0.253546  0.048977  0.133631  0.103510  0.067884  \n",
              "34  0.353553  0.298142  0.071989  0.235702  0.152145  0.119737  \n",
              "35  0.265396  0.326377  0.090065  0.221163  0.152277  0.112351  \n",
              "36  1.000000  0.252982  0.097736  0.225000  0.154919  0.152400  \n",
              "37  0.252982  1.000000  0.077267  0.355756  0.163299  0.120483  \n",
              "38  0.097736  0.077267  1.000000  0.122169  0.205036  0.124124  \n",
              "39  0.225000  0.355756  0.122169  1.000000  0.225924  0.222250  \n",
              "40  0.154919  0.163299  0.205036  0.225924  1.000000  0.262330  \n",
              "41  0.152400  0.120483  0.124124  0.222250  0.262330  1.000000  \n",
              "\n",
              "[42 rows x 42 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30af7163-8247-47a5-91f8-b4611c7c939a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.197814</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.222911</td>\n",
              "      <td>0.107676</td>\n",
              "      <td>0.269880</td>\n",
              "      <td>0.250259</td>\n",
              "      <td>0.209381</td>\n",
              "      <td>0.096309</td>\n",
              "      <td>0.166839</td>\n",
              "      <td>...</td>\n",
              "      <td>0.122868</td>\n",
              "      <td>0.159256</td>\n",
              "      <td>0.180579</td>\n",
              "      <td>0.225920</td>\n",
              "      <td>0.238352</td>\n",
              "      <td>0.269191</td>\n",
              "      <td>0.072798</td>\n",
              "      <td>0.191533</td>\n",
              "      <td>0.109897</td>\n",
              "      <td>0.129732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.197814</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.185164</td>\n",
              "      <td>0.197203</td>\n",
              "      <td>0.081650</td>\n",
              "      <td>0.146176</td>\n",
              "      <td>0.172516</td>\n",
              "      <td>0.173205</td>\n",
              "      <td>0.146059</td>\n",
              "      <td>0.161015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.149071</td>\n",
              "      <td>0.207020</td>\n",
              "      <td>0.121716</td>\n",
              "      <td>0.190347</td>\n",
              "      <td>0.154919</td>\n",
              "      <td>0.122474</td>\n",
              "      <td>0.047316</td>\n",
              "      <td>0.064550</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.065583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.185164</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.182574</td>\n",
              "      <td>0.037796</td>\n",
              "      <td>0.081200</td>\n",
              "      <td>0.287494</td>\n",
              "      <td>0.133631</td>\n",
              "      <td>0.202837</td>\n",
              "      <td>0.234255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.138013</td>\n",
              "      <td>0.095831</td>\n",
              "      <td>0.197203</td>\n",
              "      <td>0.211472</td>\n",
              "      <td>0.191237</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.043806</td>\n",
              "      <td>0.179284</td>\n",
              "      <td>0.154303</td>\n",
              "      <td>0.060718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.222911</td>\n",
              "      <td>0.197203</td>\n",
              "      <td>0.182574</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.172516</td>\n",
              "      <td>0.247083</td>\n",
              "      <td>0.291606</td>\n",
              "      <td>0.292770</td>\n",
              "      <td>0.216025</td>\n",
              "      <td>0.194404</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.233285</td>\n",
              "      <td>0.257172</td>\n",
              "      <td>0.353919</td>\n",
              "      <td>0.327327</td>\n",
              "      <td>0.310530</td>\n",
              "      <td>0.066649</td>\n",
              "      <td>0.245495</td>\n",
              "      <td>0.225374</td>\n",
              "      <td>0.166282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.107676</td>\n",
              "      <td>0.081650</td>\n",
              "      <td>0.037796</td>\n",
              "      <td>0.172516</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.179029</td>\n",
              "      <td>0.042258</td>\n",
              "      <td>0.247487</td>\n",
              "      <td>0.089443</td>\n",
              "      <td>0.140859</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136931</td>\n",
              "      <td>0.042258</td>\n",
              "      <td>0.186339</td>\n",
              "      <td>0.093250</td>\n",
              "      <td>0.221359</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.077267</td>\n",
              "      <td>0.158114</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.120483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.269880</td>\n",
              "      <td>0.146176</td>\n",
              "      <td>0.081200</td>\n",
              "      <td>0.247083</td>\n",
              "      <td>0.179029</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.121046</td>\n",
              "      <td>0.303822</td>\n",
              "      <td>0.256205</td>\n",
              "      <td>0.302614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.163430</td>\n",
              "      <td>0.121046</td>\n",
              "      <td>0.507072</td>\n",
              "      <td>0.233723</td>\n",
              "      <td>0.317038</td>\n",
              "      <td>0.179029</td>\n",
              "      <td>0.082998</td>\n",
              "      <td>0.198148</td>\n",
              "      <td>0.116941</td>\n",
              "      <td>0.172559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.250259</td>\n",
              "      <td>0.172516</td>\n",
              "      <td>0.287494</td>\n",
              "      <td>0.291606</td>\n",
              "      <td>0.042258</td>\n",
              "      <td>0.121046</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.179284</td>\n",
              "      <td>0.151186</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>...</td>\n",
              "      <td>0.154303</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.394055</td>\n",
              "      <td>0.213809</td>\n",
              "      <td>0.295804</td>\n",
              "      <td>0.081628</td>\n",
              "      <td>0.267261</td>\n",
              "      <td>0.207020</td>\n",
              "      <td>0.067884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.209381</td>\n",
              "      <td>0.173205</td>\n",
              "      <td>0.133631</td>\n",
              "      <td>0.292770</td>\n",
              "      <td>0.247487</td>\n",
              "      <td>0.303822</td>\n",
              "      <td>0.179284</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.221359</td>\n",
              "      <td>0.239046</td>\n",
              "      <td>...</td>\n",
              "      <td>0.193649</td>\n",
              "      <td>0.149404</td>\n",
              "      <td>0.368932</td>\n",
              "      <td>0.329690</td>\n",
              "      <td>0.290689</td>\n",
              "      <td>0.212132</td>\n",
              "      <td>0.068295</td>\n",
              "      <td>0.167705</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>0.141990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.096309</td>\n",
              "      <td>0.146059</td>\n",
              "      <td>0.202837</td>\n",
              "      <td>0.216025</td>\n",
              "      <td>0.089443</td>\n",
              "      <td>0.256205</td>\n",
              "      <td>0.151186</td>\n",
              "      <td>0.221359</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.277174</td>\n",
              "      <td>...</td>\n",
              "      <td>0.244949</td>\n",
              "      <td>0.151186</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.250217</td>\n",
              "      <td>0.197990</td>\n",
              "      <td>0.313050</td>\n",
              "      <td>0.069109</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>0.146059</td>\n",
              "      <td>0.107763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.166839</td>\n",
              "      <td>0.161015</td>\n",
              "      <td>0.234255</td>\n",
              "      <td>0.194404</td>\n",
              "      <td>0.140859</td>\n",
              "      <td>0.302614</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.239046</td>\n",
              "      <td>0.277174</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.154303</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.230978</td>\n",
              "      <td>0.183892</td>\n",
              "      <td>0.231626</td>\n",
              "      <td>0.197203</td>\n",
              "      <td>0.054419</td>\n",
              "      <td>0.155902</td>\n",
              "      <td>0.184017</td>\n",
              "      <td>0.090513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.140481</td>\n",
              "      <td>0.133156</td>\n",
              "      <td>0.147934</td>\n",
              "      <td>0.202567</td>\n",
              "      <td>0.163082</td>\n",
              "      <td>0.186857</td>\n",
              "      <td>0.165395</td>\n",
              "      <td>0.161443</td>\n",
              "      <td>0.233384</td>\n",
              "      <td>0.238904</td>\n",
              "      <td>...</td>\n",
              "      <td>0.208422</td>\n",
              "      <td>0.192961</td>\n",
              "      <td>0.218797</td>\n",
              "      <td>0.212905</td>\n",
              "      <td>0.165027</td>\n",
              "      <td>0.228315</td>\n",
              "      <td>0.075605</td>\n",
              "      <td>0.154713</td>\n",
              "      <td>0.186418</td>\n",
              "      <td>0.130991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.159749</td>\n",
              "      <td>0.161515</td>\n",
              "      <td>0.124611</td>\n",
              "      <td>0.182006</td>\n",
              "      <td>0.098907</td>\n",
              "      <td>0.188877</td>\n",
              "      <td>0.195047</td>\n",
              "      <td>0.163188</td>\n",
              "      <td>0.117954</td>\n",
              "      <td>0.130032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.090289</td>\n",
              "      <td>0.139320</td>\n",
              "      <td>0.098295</td>\n",
              "      <td>0.215206</td>\n",
              "      <td>0.166812</td>\n",
              "      <td>0.131876</td>\n",
              "      <td>0.063685</td>\n",
              "      <td>0.130322</td>\n",
              "      <td>0.161515</td>\n",
              "      <td>0.105925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.268261</td>\n",
              "      <td>0.203419</td>\n",
              "      <td>0.156941</td>\n",
              "      <td>0.200574</td>\n",
              "      <td>0.083045</td>\n",
              "      <td>0.178410</td>\n",
              "      <td>0.386024</td>\n",
              "      <td>0.146805</td>\n",
              "      <td>0.111417</td>\n",
              "      <td>0.163768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.113715</td>\n",
              "      <td>0.210559</td>\n",
              "      <td>0.123797</td>\n",
              "      <td>0.387202</td>\n",
              "      <td>0.183829</td>\n",
              "      <td>0.166091</td>\n",
              "      <td>0.096250</td>\n",
              "      <td>0.164133</td>\n",
              "      <td>0.203419</td>\n",
              "      <td>0.100056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.160567</td>\n",
              "      <td>0.206049</td>\n",
              "      <td>0.294817</td>\n",
              "      <td>0.174143</td>\n",
              "      <td>0.091766</td>\n",
              "      <td>0.131430</td>\n",
              "      <td>0.252059</td>\n",
              "      <td>0.178444</td>\n",
              "      <td>0.123117</td>\n",
              "      <td>0.232670</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146599</td>\n",
              "      <td>0.174503</td>\n",
              "      <td>0.136797</td>\n",
              "      <td>0.299504</td>\n",
              "      <td>0.203133</td>\n",
              "      <td>0.183533</td>\n",
              "      <td>0.053179</td>\n",
              "      <td>0.145095</td>\n",
              "      <td>0.131122</td>\n",
              "      <td>0.073708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.242272</td>\n",
              "      <td>0.081650</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.276026</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.250640</td>\n",
              "      <td>0.380319</td>\n",
              "      <td>0.318198</td>\n",
              "      <td>0.223607</td>\n",
              "      <td>0.225374</td>\n",
              "      <td>...</td>\n",
              "      <td>0.273861</td>\n",
              "      <td>0.253546</td>\n",
              "      <td>0.298142</td>\n",
              "      <td>0.466252</td>\n",
              "      <td>0.316228</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.135217</td>\n",
              "      <td>0.276699</td>\n",
              "      <td>0.244949</td>\n",
              "      <td>0.160644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.156486</td>\n",
              "      <td>0.101710</td>\n",
              "      <td>0.125553</td>\n",
              "      <td>0.200574</td>\n",
              "      <td>0.083045</td>\n",
              "      <td>0.178410</td>\n",
              "      <td>0.210559</td>\n",
              "      <td>0.205527</td>\n",
              "      <td>0.074278</td>\n",
              "      <td>0.163768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.151620</td>\n",
              "      <td>0.210559</td>\n",
              "      <td>0.154746</td>\n",
              "      <td>0.271041</td>\n",
              "      <td>0.183829</td>\n",
              "      <td>0.166091</td>\n",
              "      <td>0.080208</td>\n",
              "      <td>0.196960</td>\n",
              "      <td>0.169516</td>\n",
              "      <td>0.100056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.144463</td>\n",
              "      <td>0.036515</td>\n",
              "      <td>0.101419</td>\n",
              "      <td>0.154303</td>\n",
              "      <td>0.134164</td>\n",
              "      <td>0.160128</td>\n",
              "      <td>0.226779</td>\n",
              "      <td>0.252982</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.151186</td>\n",
              "      <td>...</td>\n",
              "      <td>0.163299</td>\n",
              "      <td>0.113389</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.291920</td>\n",
              "      <td>0.197990</td>\n",
              "      <td>0.178885</td>\n",
              "      <td>0.069109</td>\n",
              "      <td>0.212132</td>\n",
              "      <td>0.146059</td>\n",
              "      <td>0.071842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.257396</td>\n",
              "      <td>0.195180</td>\n",
              "      <td>0.180702</td>\n",
              "      <td>0.206197</td>\n",
              "      <td>0.059761</td>\n",
              "      <td>0.085592</td>\n",
              "      <td>0.404061</td>\n",
              "      <td>0.126773</td>\n",
              "      <td>0.106904</td>\n",
              "      <td>0.067344</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109109</td>\n",
              "      <td>0.202031</td>\n",
              "      <td>0.133631</td>\n",
              "      <td>0.334367</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.069264</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.146385</td>\n",
              "      <td>0.048002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.113630</td>\n",
              "      <td>0.147710</td>\n",
              "      <td>0.159545</td>\n",
              "      <td>0.228869</td>\n",
              "      <td>0.060302</td>\n",
              "      <td>0.151142</td>\n",
              "      <td>0.254824</td>\n",
              "      <td>0.170561</td>\n",
              "      <td>0.134840</td>\n",
              "      <td>0.135906</td>\n",
              "      <td>...</td>\n",
              "      <td>0.137620</td>\n",
              "      <td>0.178377</td>\n",
              "      <td>0.134840</td>\n",
              "      <td>0.309277</td>\n",
              "      <td>0.247900</td>\n",
              "      <td>0.120605</td>\n",
              "      <td>0.069890</td>\n",
              "      <td>0.143019</td>\n",
              "      <td>0.147710</td>\n",
              "      <td>0.096872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.073721</td>\n",
              "      <td>0.149071</td>\n",
              "      <td>0.207020</td>\n",
              "      <td>0.157485</td>\n",
              "      <td>0.091287</td>\n",
              "      <td>0.098058</td>\n",
              "      <td>0.115728</td>\n",
              "      <td>0.161374</td>\n",
              "      <td>0.122474</td>\n",
              "      <td>0.180021</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.077152</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.170251</td>\n",
              "      <td>0.086603</td>\n",
              "      <td>0.091287</td>\n",
              "      <td>0.070535</td>\n",
              "      <td>0.108253</td>\n",
              "      <td>0.186339</td>\n",
              "      <td>0.109985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.165228</td>\n",
              "      <td>0.139212</td>\n",
              "      <td>0.154662</td>\n",
              "      <td>0.188248</td>\n",
              "      <td>0.136399</td>\n",
              "      <td>0.219774</td>\n",
              "      <td>0.201737</td>\n",
              "      <td>0.192897</td>\n",
              "      <td>0.152499</td>\n",
              "      <td>0.211343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.155643</td>\n",
              "      <td>0.144098</td>\n",
              "      <td>0.203331</td>\n",
              "      <td>0.254385</td>\n",
              "      <td>0.194099</td>\n",
              "      <td>0.170499</td>\n",
              "      <td>0.092217</td>\n",
              "      <td>0.215666</td>\n",
              "      <td>0.222738</td>\n",
              "      <td>0.164337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.141999</td>\n",
              "      <td>0.107676</td>\n",
              "      <td>0.174456</td>\n",
              "      <td>0.204757</td>\n",
              "      <td>0.065938</td>\n",
              "      <td>0.165267</td>\n",
              "      <td>0.222911</td>\n",
              "      <td>0.186501</td>\n",
              "      <td>0.206419</td>\n",
              "      <td>0.185759</td>\n",
              "      <td>...</td>\n",
              "      <td>0.180579</td>\n",
              "      <td>0.167183</td>\n",
              "      <td>0.270310</td>\n",
              "      <td>0.245950</td>\n",
              "      <td>0.250217</td>\n",
              "      <td>0.230783</td>\n",
              "      <td>0.063685</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.134595</td>\n",
              "      <td>0.079444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.111775</td>\n",
              "      <td>0.067806</td>\n",
              "      <td>0.125553</td>\n",
              "      <td>0.200574</td>\n",
              "      <td>0.166091</td>\n",
              "      <td>0.089205</td>\n",
              "      <td>0.175466</td>\n",
              "      <td>0.176166</td>\n",
              "      <td>0.148556</td>\n",
              "      <td>0.187163</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189525</td>\n",
              "      <td>0.140372</td>\n",
              "      <td>0.154746</td>\n",
              "      <td>0.154881</td>\n",
              "      <td>0.157568</td>\n",
              "      <td>0.249136</td>\n",
              "      <td>0.064167</td>\n",
              "      <td>0.196960</td>\n",
              "      <td>0.169516</td>\n",
              "      <td>0.100056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.175762</td>\n",
              "      <td>0.177705</td>\n",
              "      <td>0.082261</td>\n",
              "      <td>0.250313</td>\n",
              "      <td>0.108821</td>\n",
              "      <td>0.129881</td>\n",
              "      <td>0.337227</td>\n",
              "      <td>0.179546</td>\n",
              "      <td>0.129777</td>\n",
              "      <td>0.143066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.198680</td>\n",
              "      <td>0.306570</td>\n",
              "      <td>0.162221</td>\n",
              "      <td>0.304430</td>\n",
              "      <td>0.206474</td>\n",
              "      <td>0.217643</td>\n",
              "      <td>0.098097</td>\n",
              "      <td>0.172062</td>\n",
              "      <td>0.207322</td>\n",
              "      <td>0.116543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.162534</td>\n",
              "      <td>0.164330</td>\n",
              "      <td>0.171157</td>\n",
              "      <td>0.243047</td>\n",
              "      <td>0.100631</td>\n",
              "      <td>0.198174</td>\n",
              "      <td>0.233884</td>\n",
              "      <td>0.213470</td>\n",
              "      <td>0.135011</td>\n",
              "      <td>0.198447</td>\n",
              "      <td>...</td>\n",
              "      <td>0.137795</td>\n",
              "      <td>0.148835</td>\n",
              "      <td>0.168763</td>\n",
              "      <td>0.281516</td>\n",
              "      <td>0.222756</td>\n",
              "      <td>0.150946</td>\n",
              "      <td>0.058316</td>\n",
              "      <td>0.119334</td>\n",
              "      <td>0.205412</td>\n",
              "      <td>0.101036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.048154</td>\n",
              "      <td>0.109545</td>\n",
              "      <td>0.169031</td>\n",
              "      <td>0.185164</td>\n",
              "      <td>0.134164</td>\n",
              "      <td>0.096077</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.221359</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.176383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.122474</td>\n",
              "      <td>0.113389</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.208514</td>\n",
              "      <td>0.169706</td>\n",
              "      <td>0.134164</td>\n",
              "      <td>0.051832</td>\n",
              "      <td>0.141421</td>\n",
              "      <td>0.146059</td>\n",
              "      <td>0.071842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.152890</td>\n",
              "      <td>0.231869</td>\n",
              "      <td>0.257603</td>\n",
              "      <td>0.254755</td>\n",
              "      <td>0.056796</td>\n",
              "      <td>0.162690</td>\n",
              "      <td>0.288009</td>\n",
              "      <td>0.180724</td>\n",
              "      <td>0.127000</td>\n",
              "      <td>0.160005</td>\n",
              "      <td>...</td>\n",
              "      <td>0.155543</td>\n",
              "      <td>0.216007</td>\n",
              "      <td>0.169334</td>\n",
              "      <td>0.317776</td>\n",
              "      <td>0.215526</td>\n",
              "      <td>0.198787</td>\n",
              "      <td>0.054856</td>\n",
              "      <td>0.202056</td>\n",
              "      <td>0.162309</td>\n",
              "      <td>0.091240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.112807</td>\n",
              "      <td>0.057027</td>\n",
              "      <td>0.052796</td>\n",
              "      <td>0.120491</td>\n",
              "      <td>0.104765</td>\n",
              "      <td>0.150047</td>\n",
              "      <td>0.118056</td>\n",
              "      <td>0.197546</td>\n",
              "      <td>0.124939</td>\n",
              "      <td>0.177084</td>\n",
              "      <td>...</td>\n",
              "      <td>0.191273</td>\n",
              "      <td>0.147570</td>\n",
              "      <td>0.182203</td>\n",
              "      <td>0.227951</td>\n",
              "      <td>0.132518</td>\n",
              "      <td>0.209529</td>\n",
              "      <td>0.053965</td>\n",
              "      <td>0.138039</td>\n",
              "      <td>0.114053</td>\n",
              "      <td>0.084149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.051333</td>\n",
              "      <td>0.116775</td>\n",
              "      <td>0.108112</td>\n",
              "      <td>0.131590</td>\n",
              "      <td>0.143019</td>\n",
              "      <td>0.068279</td>\n",
              "      <td>0.080582</td>\n",
              "      <td>0.101130</td>\n",
              "      <td>0.085280</td>\n",
              "      <td>0.107443</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087039</td>\n",
              "      <td>0.080582</td>\n",
              "      <td>0.071067</td>\n",
              "      <td>0.088911</td>\n",
              "      <td>0.120605</td>\n",
              "      <td>0.095346</td>\n",
              "      <td>0.055253</td>\n",
              "      <td>0.075378</td>\n",
              "      <td>0.116775</td>\n",
              "      <td>0.076584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.158330</td>\n",
              "      <td>0.150075</td>\n",
              "      <td>0.111154</td>\n",
              "      <td>0.177571</td>\n",
              "      <td>0.073521</td>\n",
              "      <td>0.210599</td>\n",
              "      <td>0.186411</td>\n",
              "      <td>0.155963</td>\n",
              "      <td>0.098639</td>\n",
              "      <td>0.248548</td>\n",
              "      <td>...</td>\n",
              "      <td>0.201347</td>\n",
              "      <td>0.248548</td>\n",
              "      <td>0.164399</td>\n",
              "      <td>0.171398</td>\n",
              "      <td>0.139497</td>\n",
              "      <td>0.147043</td>\n",
              "      <td>0.042606</td>\n",
              "      <td>0.145310</td>\n",
              "      <td>0.150075</td>\n",
              "      <td>0.059054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.091003</td>\n",
              "      <td>0.103510</td>\n",
              "      <td>0.223607</td>\n",
              "      <td>0.145803</td>\n",
              "      <td>0.084515</td>\n",
              "      <td>0.151307</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.179284</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>...</td>\n",
              "      <td>0.308607</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.157485</td>\n",
              "      <td>0.157622</td>\n",
              "      <td>0.106904</td>\n",
              "      <td>0.169031</td>\n",
              "      <td>0.065302</td>\n",
              "      <td>0.167038</td>\n",
              "      <td>0.207020</td>\n",
              "      <td>0.101827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.182006</td>\n",
              "      <td>0.172516</td>\n",
              "      <td>0.159719</td>\n",
              "      <td>0.262445</td>\n",
              "      <td>0.169031</td>\n",
              "      <td>0.151307</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.209165</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.270031</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.220479</td>\n",
              "      <td>0.275839</td>\n",
              "      <td>0.213809</td>\n",
              "      <td>0.253546</td>\n",
              "      <td>0.081628</td>\n",
              "      <td>0.167038</td>\n",
              "      <td>0.241523</td>\n",
              "      <td>0.169711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.122868</td>\n",
              "      <td>0.149071</td>\n",
              "      <td>0.138013</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.136931</td>\n",
              "      <td>0.163430</td>\n",
              "      <td>0.154303</td>\n",
              "      <td>0.193649</td>\n",
              "      <td>0.244949</td>\n",
              "      <td>0.154303</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.231455</td>\n",
              "      <td>0.238145</td>\n",
              "      <td>0.255377</td>\n",
              "      <td>0.173205</td>\n",
              "      <td>0.273861</td>\n",
              "      <td>0.088168</td>\n",
              "      <td>0.216506</td>\n",
              "      <td>0.186339</td>\n",
              "      <td>0.146647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.159256</td>\n",
              "      <td>0.207020</td>\n",
              "      <td>0.095831</td>\n",
              "      <td>0.233285</td>\n",
              "      <td>0.042258</td>\n",
              "      <td>0.121046</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.149404</td>\n",
              "      <td>0.151186</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>...</td>\n",
              "      <td>0.231455</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.236433</td>\n",
              "      <td>0.187083</td>\n",
              "      <td>0.253546</td>\n",
              "      <td>0.048977</td>\n",
              "      <td>0.133631</td>\n",
              "      <td>0.103510</td>\n",
              "      <td>0.067884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.180579</td>\n",
              "      <td>0.121716</td>\n",
              "      <td>0.197203</td>\n",
              "      <td>0.257172</td>\n",
              "      <td>0.186339</td>\n",
              "      <td>0.507072</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.368932</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.230978</td>\n",
              "      <td>...</td>\n",
              "      <td>0.238145</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.278019</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>0.298142</td>\n",
              "      <td>0.071989</td>\n",
              "      <td>0.235702</td>\n",
              "      <td>0.152145</td>\n",
              "      <td>0.119737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.225920</td>\n",
              "      <td>0.190347</td>\n",
              "      <td>0.211472</td>\n",
              "      <td>0.353919</td>\n",
              "      <td>0.093250</td>\n",
              "      <td>0.233723</td>\n",
              "      <td>0.394055</td>\n",
              "      <td>0.329690</td>\n",
              "      <td>0.250217</td>\n",
              "      <td>0.183892</td>\n",
              "      <td>...</td>\n",
              "      <td>0.255377</td>\n",
              "      <td>0.236433</td>\n",
              "      <td>0.278019</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.265396</td>\n",
              "      <td>0.326377</td>\n",
              "      <td>0.090065</td>\n",
              "      <td>0.221163</td>\n",
              "      <td>0.152277</td>\n",
              "      <td>0.112351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.238352</td>\n",
              "      <td>0.154919</td>\n",
              "      <td>0.191237</td>\n",
              "      <td>0.327327</td>\n",
              "      <td>0.221359</td>\n",
              "      <td>0.317038</td>\n",
              "      <td>0.213809</td>\n",
              "      <td>0.290689</td>\n",
              "      <td>0.197990</td>\n",
              "      <td>0.231626</td>\n",
              "      <td>...</td>\n",
              "      <td>0.173205</td>\n",
              "      <td>0.187083</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>0.265396</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.252982</td>\n",
              "      <td>0.097736</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.154919</td>\n",
              "      <td>0.152400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.269191</td>\n",
              "      <td>0.122474</td>\n",
              "      <td>0.188982</td>\n",
              "      <td>0.310530</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.179029</td>\n",
              "      <td>0.295804</td>\n",
              "      <td>0.212132</td>\n",
              "      <td>0.313050</td>\n",
              "      <td>0.197203</td>\n",
              "      <td>...</td>\n",
              "      <td>0.273861</td>\n",
              "      <td>0.253546</td>\n",
              "      <td>0.298142</td>\n",
              "      <td>0.326377</td>\n",
              "      <td>0.252982</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.077267</td>\n",
              "      <td>0.355756</td>\n",
              "      <td>0.163299</td>\n",
              "      <td>0.120483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.072798</td>\n",
              "      <td>0.047316</td>\n",
              "      <td>0.043806</td>\n",
              "      <td>0.066649</td>\n",
              "      <td>0.077267</td>\n",
              "      <td>0.082998</td>\n",
              "      <td>0.081628</td>\n",
              "      <td>0.068295</td>\n",
              "      <td>0.069109</td>\n",
              "      <td>0.054419</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088168</td>\n",
              "      <td>0.048977</td>\n",
              "      <td>0.071989</td>\n",
              "      <td>0.090065</td>\n",
              "      <td>0.097736</td>\n",
              "      <td>0.077267</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.122169</td>\n",
              "      <td>0.205036</td>\n",
              "      <td>0.124124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.191533</td>\n",
              "      <td>0.064550</td>\n",
              "      <td>0.179284</td>\n",
              "      <td>0.245495</td>\n",
              "      <td>0.158114</td>\n",
              "      <td>0.198148</td>\n",
              "      <td>0.267261</td>\n",
              "      <td>0.167705</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>0.155902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216506</td>\n",
              "      <td>0.133631</td>\n",
              "      <td>0.235702</td>\n",
              "      <td>0.221163</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.355756</td>\n",
              "      <td>0.122169</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.225924</td>\n",
              "      <td>0.222250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.109897</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.154303</td>\n",
              "      <td>0.225374</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.116941</td>\n",
              "      <td>0.207020</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>0.146059</td>\n",
              "      <td>0.184017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.186339</td>\n",
              "      <td>0.103510</td>\n",
              "      <td>0.152145</td>\n",
              "      <td>0.152277</td>\n",
              "      <td>0.154919</td>\n",
              "      <td>0.163299</td>\n",
              "      <td>0.205036</td>\n",
              "      <td>0.225924</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.262330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.129732</td>\n",
              "      <td>0.065583</td>\n",
              "      <td>0.060718</td>\n",
              "      <td>0.166282</td>\n",
              "      <td>0.120483</td>\n",
              "      <td>0.172559</td>\n",
              "      <td>0.067884</td>\n",
              "      <td>0.141990</td>\n",
              "      <td>0.107763</td>\n",
              "      <td>0.090513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146647</td>\n",
              "      <td>0.067884</td>\n",
              "      <td>0.119737</td>\n",
              "      <td>0.112351</td>\n",
              "      <td>0.152400</td>\n",
              "      <td>0.120483</td>\n",
              "      <td>0.124124</td>\n",
              "      <td>0.222250</td>\n",
              "      <td>0.262330</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42 rows × 42 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30af7163-8247-47a5-91f8-b4611c7c939a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30af7163-8247-47a5-91f8-b4611c7c939a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30af7163-8247-47a5-91f8-b4611c7c939a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "dUTkTNrw81hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot = ChatBot(\n",
        "    'ReporterBot',\n",
        "    storage_adapter='chatterbot.storage.SQLStorageAdapter',\n",
        "    database_uri='sqlite:///database.sqlite3'\n",
        "    )\n",
        "    \n",
        "chat = ListTrainer(bot)\n",
        "chat.train(all_senteces)"
      ],
      "metadata": {
        "id": "fmuH4z1481I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  response = bot.get_response(input(\"Pergunta: \"))\n",
        "  print(\"[Confiança na resposta: {:}%]\\nResposta:\".format(100*float(response.confidence)), response)"
      ],
      "metadata": {
        "id": "Gp9JrjKE-fps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}