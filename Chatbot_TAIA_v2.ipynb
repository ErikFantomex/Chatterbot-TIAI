{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCCIsco0CKISw5gUz0ot8O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErikFantomex/Chatterbot-TIAI/blob/main/Chatbot_TAIA_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "U_GL2agb7Ge5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chatterbot\n",
        "!pip install chatterbot-corpus"
      ],
      "metadata": {
        "id": "dhNbicu98_Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from chatterbot import ChatBot\n",
        "from chatterbot.trainers import ListTrainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aln2wID58Aqw",
        "outputId": "be51c5c2-6965-4646-f3ed-fca006d31eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-SsT3pI6_Pn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#iniciamos\n",
        "url = 'https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html'\n",
        "page = requests.get(url)\n",
        "page = page.text\n",
        "soup = BeautifulSoup(page, features=\"lxml\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.decompose()\n",
        "raw_text  = soup.get_text()\n",
        "\n",
        "cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(\\1>)\", \"\", raw_text.strip())\n",
        "cleaned = re.sub(r\"(?s)[\\n]?\", \"\", cleaned)\n",
        "cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
        "cleaned = re.sub(r\" \", \" \", cleaned)\n",
        "cleaned = re.sub(r\"  \", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\n\", \" \", cleaned)\n",
        "cleaned = cleaned.strip()\n",
        "\n",
        "tokens = nltk.sent_tokenize(cleaned)\n",
        "for token in tokens:\n",
        "  print('-> ', token, '\\n')\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html'\n",
        "page = requests.get(url)\n",
        "page = page.text\n",
        "soup = BeautifulSoup(page, features=\"lxml\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.decompose()\n",
        "raw_text  = soup.get_text()\n",
        "\n",
        "cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(\\1>)\", \"\", raw_text.strip())\n",
        "cleaned = re.sub(r\"(?s)[\\n]?\", \"\", cleaned)\n",
        "cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
        "cleaned = re.sub(r\" \", \" \", cleaned)\n",
        "cleaned = re.sub(r\"  \", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\n\", \" \", cleaned)\n",
        "cleaned = re.sub(\"'\", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\\"\", \" \", cleaned)\n",
        "cleaned = cleaned.strip()\n",
        "     \n"
      ],
      "metadata": {
        "id": "QVzit9-s74GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorizacion"
      ],
      "metadata": {
        "id": "d4j4-zYv7dCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = nltk.sent_tokenize(cleaned)\n",
        "word_tokens = nltk.word_tokenize(cleaned)"
      ],
      "metadata": {
        "id": "0wUo5OrA7KXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "total_ocurrences = {}\n",
        "occured_in_n_docs = {}\n",
        "\n",
        "for sentence in sent_tokens:\n",
        "  for word in word_tokens:\n",
        " \n",
        "    c = sentence.count(word)\n",
        " \n",
        "    try:\n",
        "      total_ocurrences[word] += c\n",
        "    except KeyError:\n",
        "      total_ocurrences[word] = c \n",
        " \n",
        "    if c > 0:\n",
        "      try:\n",
        "        occured_in_n_docs[word] += 1\n",
        "      except KeyError:\n",
        "        occured_in_n_docs[word] = 1\n",
        " \n",
        "\n",
        "tfidf = []\n",
        "i = 0\n",
        "for sentence in sent_tokens:\n",
        "  w = {\"sentence_n\": i}\n",
        "  for word in word_tokens:\n",
        "    # w = tf * log(N/df)\n",
        "    tf = sentence.count(word)\n",
        "    N = total_ocurrences[word]\n",
        "    df = occured_in_n_docs[word]\n",
        "    if tf == 0:\n",
        "      w[word] = 0\n",
        "    else:\n",
        "      w[word] = tf * math.log(N/df)\n",
        "  tfidf.append(w)\n",
        "  i += 1\n"
      ],
      "metadata": {
        "id": "DrAOzTfU7wCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(tfidf)\n",
        "     "
      ],
      "metadata": {
        "id": "ZGH_s91L78wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similaridad de coseno"
      ],
      "metadata": {
        "id": "2s18aIkx8Pp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html'\n",
        "page = requests.get(url)\n",
        "page = page.text\n",
        "soup = BeautifulSoup(page, features=\"lxml\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.decompose()\n",
        "raw_text  = soup.get_text()\n",
        "\n",
        "cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(\\1>)\", \"\", raw_text.strip())\n",
        "cleaned = re.sub(r\"(?s)[\\n]?\", \"\", cleaned)\n",
        "cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
        "cleaned = re.sub(r\" \", \" \", cleaned)\n",
        "cleaned = re.sub(r\"  \", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\n\", \" \", cleaned)\n",
        "cleaned = re.sub(\"'\", \" \", cleaned)\n",
        "cleaned = re.sub(\"\\\"\", \" \", cleaned)\n",
        "cleaned = cleaned.strip()\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(cleaned)"
      ],
      "metadata": {
        "id": "zxhtZlOR8LXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "similaridades = []\n",
        "for sentece in sent_tokens:\n",
        "  row = []\n",
        "  A = {w for w in nltk.word_tokenize(sentece)}\n",
        "  for another_sentence in sent_tokens:\n",
        "    l1 =[]\n",
        "    l2 =[]\n",
        "    B = {w for w in nltk.word_tokenize(another_sentence)}\n",
        "    v = A.union(B) \n",
        "    for w in v:\n",
        "      l1.append(1 if w in A else 0)\n",
        "      l2.append(1 if w in B else 0)\n",
        "    c = 0\n",
        "    for i in range(len(v)):\n",
        "        c+= l1[i]*l2[i]\n",
        "    row.append(c / float((sum(l1) * sum(l2)) ** 0.5))\n",
        "  similaridades.append(row)"
      ],
      "metadata": {
        "id": "oDouE19k8Uxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(similaridades)"
      ],
      "metadata": {
        "id": "_bNokIF-8ZBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "dUTkTNrw81hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_list = [\n",
        "            \"https://es.wikipedia.org/wiki/Ucr%C3%A2nia\",\n",
        "            \"https://es.wikipedia.org/wiki/Rusia\",\n",
        "            \"https://es.wikipedia.org/wiki/Invasi%C3%B3n_rusa_de_Ucrania_de_2022\",\n",
        "            \"https://es.wikipedia.org/wiki/Guerra_em_Donbas\",\n",
        "            \"https://es.wikipedia.org/wiki/Crimea\",\n",
        "            \"https://cnnespanol.cnn.com/2022/12/08/ultima-hora-noticias-guerra-rusia-ucrania-orix-58/\",\n",
        "            \"https://cnnespanol.cnn.com/2022/11/09/jerson-rusia-guerra-ucrania-orix/\",\n",
        "            \"https://cnnespanol.cnn.com/2022/12/05/ultima-hora-noticias-guerra-rusia-ucrania-orix-55/\",\n",
        "            \"https://cnnespanol.cnn.com/\",\n",
        "            \"https://www.nytimes.com/es/\",\n",
        "            \"https://stackoverflow.com/questions/50454716/cannot-properly-import-the-chatterbot-and-its-corpus\"\n",
        "            ]\n",
        "\n",
        "\n",
        "all_senteces = []\n",
        "for url in url_list:\n",
        "  page = requests.get(url)\n",
        "  page = page.text\n",
        "  soup = BeautifulSoup(page, features=\"lxml\")\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "      script.decompose()\n",
        "  raw_text  = soup.get_text()\n",
        "\n",
        "  cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(\\1>)\", \"\", raw_text.strip())\n",
        "  cleaned = re.sub(r\"(?s)[\\n]?\", \"\", cleaned)\n",
        "  cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
        "  cleaned = re.sub(r\" \", \" \", cleaned)\n",
        "  cleaned = re.sub(r\"  \", \" \", cleaned)\n",
        "  cleaned = re.sub(\"\\n\", \" \", cleaned)\n",
        "  cleaned = re.sub(\"'\", \" \", cleaned)\n",
        "  cleaned = re.sub(\"\\\"\", \" \", cleaned)\n",
        "  cleaned = cleaned.strip()\n",
        "  sent_tokens = nltk.sent_tokenize(cleaned)\n",
        "  for token in sent_tokens:\n",
        "    all_senteces.append(token)"
      ],
      "metadata": {
        "id": "Ay6getNxAbfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = ChatBot(\n",
        "    'BotRepor',\n",
        "    storage_adapter='chatterbot.storage.SQLStorageAdapter',\n",
        "    database_uri='sqlite:///database.sqlite3'\n",
        "    )\n",
        "    \n",
        "chat = ListTrainer(bot)\n",
        "chat.train(all_senteces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmuH4z1481I3",
        "outputId": "158ff4f2-a1d0-44ab-d216-bb363d4377bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List Trainer: [                    ] 0%"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List Trainer: [####################] 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  response = bot.get_response(input(\"Pregunta: \"))\n",
        "  print(\"[Confianza en la respuesta: {:}%]\\nRespuesta:\".format(100*float(response.confidence)), response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "Gp9JrjKE-fps",
        "outputId": "6d2fbf80-90c6-4b17-fcee-a868e2dcdf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta: rusia\n",
            "[Confianza en la respuesta: 7.000000000000001%]\n",
            "Respuesta: LRT.\n",
            "Pregunta: >Hora dcdmx\n",
            "[Confianza en la respuesta: 0.0%]\n",
            "Respuesta: [368] Em 27 de fevereiro, mais de 100 mil pessoas se reuniram em Berlim para protestar contra a invasão da Rússia.\n",
            "Pregunta: Hora cdmx\n",
            "[Confianza en la respuesta: 0.0%]\n",
            "Respuesta: [130]​Se han empleado varias tácticas de resistencia civil desarmada en toda Ucrania.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-4a80078c45ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pregunta: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Confianza en la respuesta: {:}%]\\nRespuesta:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}